"""
æ¨¡å— 4: RAG åˆæˆå¼•æ“ (RAG Synthesis Engine)
èŒè´£ï¼šç»“åˆç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢åˆ°çš„è¯æ®ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
"""

import logging
from typing import Dict, Any, List, Optional

from ..phase3_agent_interaction import LLMGateway

logger = logging.getLogger(__name__)


class RAGSynthesisEngine:
    """RAG åˆæˆå¼•æ“"""
    
    def __init__(self, 
                 model_name: str = 'gemini-2.5-flash-lite',
                 temperature: float = 0.3,
                 max_output_tokens: int = 512):
        """
        åˆå§‹åŒ– RAG åˆæˆå¼•æ“
        
        Args:
            model_name: Gemini æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_output_tokens: æœ€å¤§è¾“å‡º token æ•°
        """
        self.llm_gateway = LLMGateway(
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens
        )
        
        logger.info(f"âœ… RAGSynthesisEngine åˆå§‹åŒ–å®Œæˆ: {model_name}")
    
    def synthesize(self, user_query: str, 
                   retrieved_evidence: List[Dict[str, Any]],
                   query_obj: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆæˆæœ€ç»ˆå›ç­”
        
        Args:
            user_query: ç”¨æˆ·åŸå§‹é—®é¢˜
            retrieved_evidence: æ£€ç´¢åˆ°çš„è¯æ®åˆ—è¡¨
            query_obj: æŸ¥è¯¢å¯¹è±¡ï¼ˆæ¥è‡ª QueryParserï¼‰
        
        Returns:
            å›ç­”å­—å…¸: {
                'answer': str,  # æœ€ç»ˆå›ç­”æ–‡æœ¬
                'evidence_count': int,  # ä½¿ç”¨çš„è¯æ®æ•°é‡
                'has_images': bool,  # æ˜¯å¦åŒ…å«å›¾ç‰‡
                'images': List[str]  # å›¾ç‰‡ URL åˆ—è¡¨
            }
        """
        if not retrieved_evidence:
            return self._generate_no_result_answer(user_query)
        
        # æ„å»º Prompt
        system_prompt = self._build_system_prompt(query_obj)
        user_prompt = self._build_user_prompt(user_query, retrieved_evidence, query_obj)
        
        # è°ƒç”¨ LLM
        logger.info(f"ğŸ¤– è°ƒç”¨ LLM ç”Ÿæˆå›ç­”...")
        try:
            answer_text = self.llm_gateway.generate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = []
            for evidence in retrieved_evidence:
                if evidence.get('type') == 'detail':
                    for appearance in evidence.get('appearances', []):
                        if appearance.get('snapshot_url'):
                            images.append(appearance['snapshot_url'])
            
            result = {
                'answer': answer_text.strip(),
                'evidence_count': len(retrieved_evidence),
                'has_images': len(images) > 0,
                'images': images
            }
            
            logger.info(f"âœ… RAG åˆæˆå®Œæˆ: {len(answer_text)} å­—ç¬¦, {len(images)} å¼ å›¾ç‰‡")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            return self._generate_fallback_answer(user_query, retrieved_evidence)
    
    def _build_system_prompt(self, query_obj: Dict[str, Any]) -> str:
        """
        æ„å»º System Prompt
        
        Args:
            query_obj: æŸ¥è¯¢å¯¹è±¡
        
        Returns:
            System Prompt å­—ç¬¦ä¸²
        """
        intent = query_obj.get('intent', 'general')
        
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¶åº­å®‰é˜²ç³»ç»Ÿçš„é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æ£€ç´¢åˆ°çš„æ•°æ®åº“è®°å½•ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”
2. åŸºäºæ£€ç´¢åˆ°çš„è¯æ®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
4. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€äººæ€§åŒ–
5. å¦‚æœæ¶‰åŠæ—¶é—´ï¼Œä½¿ç”¨å…·ä½“çš„æ—¶é—´æ ¼å¼ï¼ˆå¦‚"2025å¹´9æœˆ1æ—¥ 18:00"ï¼‰
"""
        
        # æ ¹æ®æ„å›¾æ·»åŠ ç‰¹å®šè¦æ±‚
        if intent == 'describe_appearance':
            base_prompt += "\n6. å¦‚æœç”¨æˆ·è¯¢é—®è¡£ç€ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ body_embedding ç‰¹å¾æè¿°ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œæˆ–è¯´æ˜æ— æ³•ä»å½“å‰æ•°æ®ä¸­ç¡®å®šå…·ä½“è¡£ç€ã€‚"
        elif intent == 'query_time':
            base_prompt += "\n6. å¦‚æœç”¨æˆ·è¯¢é—®æ—¶é—´ï¼Œæä¾›å…·ä½“çš„æ—¶é—´ä¿¡æ¯ã€‚"
        elif intent == 'query_location':
            base_prompt += "\n6. å¦‚æœç”¨æˆ·è¯¢é—®ä½ç½®ï¼Œæä¾›å…·ä½“çš„æ‘„åƒå¤´ä½ç½®ä¿¡æ¯ã€‚"
        
        return base_prompt
    
    def _build_user_prompt(self, user_query: str,
                          retrieved_evidence: List[Dict[str, Any]],
                          query_obj: Dict[str, Any]) -> str:
        """
        æ„å»º User Prompt
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_evidence: æ£€ç´¢åˆ°çš„è¯æ®
            query_obj: æŸ¥è¯¢å¯¹è±¡
        
        Returns:
            User Prompt å­—ç¬¦ä¸²
        """
        prompt_parts = []
        
        prompt_parts.append(f"ç”¨æˆ·é—®é¢˜ï¼š{user_query}\n")
        prompt_parts.append("æ£€ç´¢åˆ°çš„è¯æ®ï¼š\n")
        
        # æ ¼å¼åŒ–è¯æ®
        for idx, evidence in enumerate(retrieved_evidence[:5], 1):  # æœ€å¤šä½¿ç”¨å‰5æ¡è¯æ®
            if evidence.get('type') == 'summary':
                prompt_parts.append(f"\n[{idx}] æ¯æ—¥æ€»ç»“:")
                prompt_parts.append(f"   æ—¥æœŸ: {evidence.get('summary_date')}")
                prompt_parts.append(f"   å†…å®¹: {evidence.get('summary_text')}")
            elif evidence.get('type') == 'detail':
                prompt_parts.append(f"\n[{idx}] äº‹ä»¶è®°å½•:")
                prompt_parts.append(f"   æ—¶é—´: {evidence.get('start_time')}")
                prompt_parts.append(f"   ä½ç½®: {evidence.get('camera_location')}")
                prompt_parts.append(f"   æè¿°: {evidence.get('llm_description')}")
                
                # æ·»åŠ äººç‰©ä¿¡æ¯
                appearances = evidence.get('appearances', [])
                if appearances:
                    prompt_parts.append(f"   æ¶‰åŠäººç‰©:")
                    for app in appearances:
                        person_name = app.get('person_name', f"Person_{app.get('person_id')}")
                        match_method = app.get('match_method', 'unknown')
                        prompt_parts.append(f"     - {person_name} (è¯†åˆ«æ–¹å¼: {match_method})")
        
        prompt_parts.append("\nè¯·æ ¹æ®ä»¥ä¸Šè¯æ®ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _generate_no_result_answer(self, user_query: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ— ç»“æœå›ç­”
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å›ç­”å­—å…¸
        """
        answer = f"æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„è®°å½•ã€‚è¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶ï¼Œæ¯”å¦‚ï¼š\n- æ£€æŸ¥æ—¥æœŸæ˜¯å¦æ­£ç¡®\n- ç¡®è®¤äººç‰©åç§°\n- ä½¿ç”¨ä¸åŒçš„å…³é”®è¯"
        
        return {
            'answer': answer,
            'evidence_count': 0,
            'has_images': False,
            'images': []
        }
    
    def _generate_fallback_answer(self, user_query: str,
                                 retrieved_evidence: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå…œåº•å›ç­”ï¼ˆå½“ LLM è°ƒç”¨å¤±è´¥æ—¶ï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_evidence: æ£€ç´¢åˆ°çš„è¯æ®
        
        Returns:
            å›ç­”å­—å…¸
        """
        if not retrieved_evidence:
            return self._generate_no_result_answer(user_query)
        
        # ç®€å•æ ¼å¼åŒ–è¯æ®
        evidence_summary = []
        for evidence in retrieved_evidence[:3]:
            if evidence.get('type') == 'detail':
                time_str = evidence.get('start_time', 'æœªçŸ¥æ—¶é—´')
                desc = evidence.get('llm_description', 'æ— æè¿°')
                evidence_summary.append(f"- {time_str}: {desc[:50]}...")
        
        answer = f"æ ¹æ®æ£€ç´¢åˆ°çš„ {len(retrieved_evidence)} æ¡è®°å½•ï¼Œç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š\n" + "\n".join(evidence_summary)
        
        return {
            'answer': answer,
            'evidence_count': len(retrieved_evidence),
            'has_images': False,
            'images': []
        }

