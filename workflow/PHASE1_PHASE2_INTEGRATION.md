# Phase 1 + Phase 2 é›†æˆæŒ‡å—

## ğŸ”— è¿æ¥æ–¹å¼

ç¬¬ä¸€é˜¶æ®µï¼ˆPhase 1ï¼‰çš„è¾“å‡ºæ˜¯ `Clip_Obj` åˆ—è¡¨ï¼Œç›´æ¥ä½œä¸ºç¬¬äºŒé˜¶æ®µï¼ˆPhase 2ï¼‰çš„è¾“å…¥ã€‚

## ğŸ“Š æ•°æ®æµ

```
Phase 1: CV_Pipeline
  â†“
è¾“å‡º: List[Clip_Obj]
  â†“
Phase 2: Event_Fusion_Pipeline
  â†“
è¾“å‡º: List[Global_Event]
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨é›†æˆè„šæœ¬ï¼ˆæ¨èï¼‰

```bash
python workflow/integrate_phase1_phase2.py
```

è¿™ä¼šè‡ªåŠ¨è¿è¡Œ Phase 1 â†’ Phase 2 çš„å®Œæ•´æµç¨‹ã€‚

### æ–¹æ³•2ï¼šæ‰‹åŠ¨è¿æ¥

```python
from workflow import CV_Pipeline, Event_Fusion_Pipeline

# ========== Phase 1 ==========
cv_pipeline = CV_Pipeline(
    dataset_json_path='memories_ai_benchmark/long_mem_dataset.json',
    videos_base_dir='memories_ai_benchmark/videos'
)

# å¤„ç†è§†é¢‘ï¼Œç”Ÿæˆ Clip_Obj åˆ—è¡¨
clip_objs = cv_pipeline.process_all_clips(max_clips=10)

# ========== Phase 2 ==========
fusion_pipeline = Event_Fusion_Pipeline(time_threshold=60)

# å°† Phase 1 çš„è¾“å‡ºä¼ é€’ç»™ Phase 2
global_events = fusion_pipeline.run(clip_objs)

# ========== å¤„ç†ç»“æœ ==========
for event in global_events:
    print(f"äº‹ä»¶: {event['start_time']} ~ {event['end_time']}")
    print(f"äººç‰©: {event['people']}")
    print(f"Prompt: {event['prompt_text']}")
```

## ğŸ“‹ æ•°æ®æ ¼å¼

### Phase 1 è¾“å‡ºï¼šClip_Obj

```python
{
    'time': datetime(2025, 9, 1, 9, 0, 0),
    'cam': 'doorbell',
    'people_detected': [
        [  # ç¬¬1å¸§
            {
                'person_id': 1,
                'role': 'family',
                'method': 'face',
                'bbox': (100, 100, 200, 300),
                'confidence': 0.9,
                'frame_idx': 0
            }
        ],
        [  # ç¬¬2å¸§
            ...
        ]
    ]
}
```

### Phase 2 è¾“å‡ºï¼šGlobal_Event

```python
{
    'start_time': datetime(2025, 9, 1, 9, 0, 0),
    'end_time': datetime(2025, 9, 1, 9, 0, 30),
    'duration': 30.0,
    'cameras': ['doorbell', 'indoor_living'],
    'people': {1, 2},
    'people_info': {
        1: {
            'person_id': 1,
            'role': 'family',
            'method': 'face',
            'first_seen': datetime(...),
            'last_seen': datetime(...),
            'cameras': ['doorbell', 'indoor_living']
        }
    },
    'clips': [...],  # åŸå§‹ Clip_Obj åˆ—è¡¨
    'keyframes': {
        1: {
            'bbox': (100, 100, 200, 300),
            'confidence': 0.9,
            'method': 'face',
            'clip_time': datetime(...),
            'cam': 'doorbell'
        }
    },
    'prompt_text': 'Plaintextæ—¶é—´çº¿ï¼š\n- 09:00:00 [doorbell]: å®¶äºº(Person_1) å‡ºç°\n...',
    'clip_count': 3
}
```

## ğŸ’¡ å…³é”®ç‚¹

### 1. æ•°æ®ä¼ é€’

Phase 1 çš„è¾“å‡ºï¼ˆ`clip_objs`ï¼‰ç›´æ¥ä½œä¸º Phase 2 çš„è¾“å…¥ï¼š

```python
clip_objs = cv_pipeline.process_all_clips(...)  # Phase 1
global_events = fusion_pipeline.run(clip_objs)  # Phase 2
```

### 2. æ•°æ®æ ¼å¼å…¼å®¹

Phase 1 è¾“å‡ºçš„ `Clip_Obj` æ ¼å¼å®Œå…¨ç¬¦åˆ Phase 2 çš„è¾“å…¥è¦æ±‚ï¼Œæ— éœ€è½¬æ¢ã€‚

### 3. æ‰¹é‡å¤„ç†

å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªè§†é¢‘ï¼š

```python
# Phase 1: å¤„ç†æ‰€æœ‰è§†é¢‘
clip_objs = cv_pipeline.process_all_clips()  # å¤„ç†å…¨éƒ¨665ä¸ªè§†é¢‘

# Phase 2: ä¸€æ¬¡æ€§èåˆæ‰€æœ‰äº‹ä»¶
global_events = fusion_pipeline.run(clip_objs)
```

### 4. å¢é‡å¤„ç†

ä¹Ÿå¯ä»¥åˆ†æ‰¹å¤„ç†ï¼š

```python
# åˆ†æ‰¹å¤„ç†
for batch in range(0, total_videos, 100):
    clip_objs = cv_pipeline.process_all_clips(max_clips=100, start_idx=batch)
    global_events = fusion_pipeline.run(clip_objs)
    # ä¿å­˜æˆ–å¤„ç† global_events
```

## ğŸ” å®Œæ•´ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""å®Œæ•´çš„ Phase 1 â†’ Phase 2 æµç¨‹"""

from workflow import CV_Pipeline, Event_Fusion_Pipeline
from pathlib import Path

# 1. åˆå§‹åŒ– Phase 1
cv_pipeline = CV_Pipeline(
    dataset_json_path='memories_ai_benchmark/long_mem_dataset.json',
    videos_base_dir='memories_ai_benchmark/videos',
    enable_tracking=True  # å¯ç”¨è·Ÿè¸ªä¼˜åŒ–
)

# 2. è¿è¡Œ Phase 1
print("è¿è¡Œ Phase 1...")
clip_objs = cv_pipeline.process_all_clips(max_clips=10)
print(f"âœ… Phase 1 å®Œæˆ: {len(clip_objs)} ä¸ª Clip_Obj")

# 3. åˆå§‹åŒ– Phase 2
fusion_pipeline = Event_Fusion_Pipeline(time_threshold=60)

# 4. è¿è¡Œ Phase 2
print("\nè¿è¡Œ Phase 2...")
global_events = fusion_pipeline.run(clip_objs)
print(f"âœ… Phase 2 å®Œæˆ: {len(global_events)} ä¸ªå…¨å±€äº‹ä»¶")

# 5. å¤„ç†ç»“æœ
for idx, event in enumerate(global_events, 1):
    print(f"\näº‹ä»¶ #{idx}:")
    print(f"  æ—¶é—´: {event['start_time']} ~ {event['end_time']}")
    print(f"  äººç‰©: {list(event['people'])}")
    print(f"  Clip æ•°: {event['clip_count']}")
    print(f"  Prompt: {event['prompt_text'][:100]}...")
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: Phase 1 æ²¡æœ‰è¾“å‡ºæ€ä¹ˆåŠï¼Ÿ

A: æ£€æŸ¥ï¼š
1. è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
2. æ•°æ®åº“æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆPhase 0ï¼‰
3. èº«ä½“ç‰¹å¾ç¼“å­˜æ˜¯å¦å·²åˆ›å»º

### Q: Phase 2 æ²¡æœ‰ç”Ÿæˆäº‹ä»¶æ€ä¹ˆåŠï¼Ÿ

A: æ£€æŸ¥ï¼š
1. Clip_Obj åˆ—è¡¨æ˜¯å¦ä¸ºç©º
2. æ—¶é—´é˜ˆå€¼æ˜¯å¦è®¾ç½®åˆç†ï¼ˆé»˜è®¤60ç§’ï¼‰
3. Clip ä¹‹é—´æ˜¯å¦æœ‰æ—¶é—´é—´éš”

### Q: å¦‚ä½•è°ƒæ•´èåˆç­–ç•¥ï¼Ÿ

A: ä¿®æ”¹ `Event_Fusion_Pipeline` çš„å‚æ•°ï¼š

```python
fusion_pipeline = Event_Fusion_Pipeline(
    time_threshold=120  # å¢åŠ åˆ°120ç§’
)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `workflow/integrate_phase1_phase2.py` - é›†æˆè„šæœ¬
- `workflow/phase1_cv_scanning/README.md` - Phase 1 æ–‡æ¡£
- `workflow/phase2_event_fusion/README.md` - Phase 2 æ–‡æ¡£

