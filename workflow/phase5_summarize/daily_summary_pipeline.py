"""
ä¸» Pipeline: æ¯æ—¥æ€»ç»“ç”Ÿæˆ Pipeline
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„æ¯æ—¥æ€»ç»“ç”Ÿæˆæµç¨‹
"""

import logging
from typing import Optional, Dict, Any, List
from datetime import datetime

from .query_engine import QueryEngine
from .narrative_aggregator import NarrativeAggregator
from .insight_engine import InsightEngine
from .archive_persister import ArchivePersister

logger = logging.getLogger(__name__)


class Daily_Summary_Pipeline:
    """æ¯æ—¥æ€»ç»“ç”Ÿæˆ Pipeline"""
    
    def __init__(self, 
                 db_config: Optional[Dict[str, str]] = None,
                 model_name: str = 'gemini-2.5-flash-lite',
                 temperature: float = 0.3,
                 max_output_tokens: int = 512):
        """
        åˆå§‹åŒ–æ¯æ—¥æ€»ç»“ Pipeline
        
        Args:
            db_config: æ•°æ®åº“è¿æ¥é…ç½®
            model_name: LLM æ¨¡å‹åç§°
            temperature: LLM æ¸©åº¦å‚æ•°
            max_output_tokens: LLM æœ€å¤§è¾“å‡º token æ•°
        """
        logger.info("=" * 60)
        logger.info("åˆå§‹åŒ– Daily Summary Pipeline (ç¬¬äº”é˜¶æ®µ)")
        logger.info("=" * 60)
        
        self.query_engine = QueryEngine(db_config)
        self.aggregator = NarrativeAggregator()
        self.insight_engine = InsightEngine(
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens
        )
        self.persister = ArchivePersister(db_config)
        
        logger.info("âœ… Daily Summary Pipeline åˆå§‹åŒ–å®Œæˆ")
    
    def run_for_date(self, target_date: str, force_update: bool = False) -> Optional[int]:
        """
        å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ€»ç»“
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD' (å¦‚ '2025-09-01')
            force_update: å¦‚æœä¸º Trueï¼Œå³ä½¿å·²å­˜åœ¨æ€»ç»“ä¹Ÿä¼šé‡æ–°ç”Ÿæˆ
        
        Returns:
            ä¿å­˜çš„è®°å½•IDï¼ˆå¦‚æœæˆåŠŸï¼‰ï¼Œå¦åˆ™è¿”å› None
        """
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹å¤„ç†æ—¥æœŸ: {target_date}")
        logger.info("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ€»ç»“
        if not force_update:
            existing_summary = self.persister.get_summary(target_date)
            if existing_summary:
                logger.info(f"âœ… æ—¥æœŸ {target_date} å·²æœ‰æ€»ç»“ï¼Œè·³è¿‡ç”Ÿæˆï¼ˆä½¿ç”¨ force_update=True å¼ºåˆ¶æ›´æ–°ï¼‰")
                logger.info(f"   ç°æœ‰æ€»ç»“: {existing_summary['summary_text'][:100]}...")
                return existing_summary['id']
        
        # 1. æŸ¥è¯¢æ•°æ®ï¼ˆæ¨¡å— 1ï¼‰
        logger.info("[æ­¥éª¤ 1] æŸ¥è¯¢æ•°æ®åº“äº‹ä»¶...")
        events = self.query_engine.fetch_events(target_date)
        
        if not events:
            logger.warning(f"âš ï¸  æ—¥æœŸ {target_date} æ²¡æœ‰äº‹ä»¶è®°å½•")
            return None
        
        logger.info(f"âœ… æ‰¾åˆ° {len(events)} ä¸ªäº‹ä»¶")
        
        # 2. æ ¼å¼åŒ–æ—¶é—´çº¿ï¼ˆæ¨¡å— 2ï¼‰
        logger.info("[æ­¥éª¤ 2] æ ¼å¼åŒ–æ—¶é—´çº¿...")
        timeline_text = self.aggregator.format_timeline(events)
        
        # æ£€æŸ¥ token é™åˆ¶
        if not self.aggregator.check_token_limit(timeline_text):
            logger.warning("âš ï¸  æ—¶é—´çº¿æ–‡æœ¬è¿‡é•¿ï¼Œå¯èƒ½ä¼šå½±å“ LLM å¤„ç†")
        
        logger.info(f"âœ… æ—¶é—´çº¿æ ¼å¼åŒ–å®Œæˆ: {len(timeline_text)} å­—ç¬¦")
        
        # 3. LLM ç”Ÿæˆæ€»ç»“ï¼ˆæ¨¡å— 3ï¼‰
        logger.info("[æ­¥éª¤ 3] è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“...")
        summary_text = self.insight_engine.analyze(timeline_text, target_date)
        
        logger.info(f"âœ… LLM æ€»ç»“ç”Ÿæˆå®Œæˆ: {len(summary_text)} å­—ç¬¦")
        logger.info(f"   æ€»ç»“é¢„è§ˆ: {summary_text[:150]}...")
        
        # 4. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ¨¡å— 4ï¼‰
        logger.info("[æ­¥éª¤ 4] ä¿å­˜æ€»ç»“åˆ°æ•°æ®åº“...")
        record_id = self.persister.save(
            summary_date=target_date,
            summary_text=summary_text,
            total_events=len(events)
        )
        
        logger.info("=" * 60)
        logger.info(f"âœ… æ—¥æœŸ {target_date} å¤„ç†å®Œæˆ: record_id={record_id}")
        logger.info("=" * 60)
        
        return record_id
    
    def run_batch(self, date_list: Optional[List[str]] = None, force_update: bool = False) -> Dict[str, int]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ—¥æœŸçš„æ€»ç»“
        
        Args:
            date_list: æ—¥æœŸåˆ—è¡¨ã€‚å¦‚æœä¸º Noneï¼Œåˆ™å¤„ç†æ•°æ®åº“ä¸­æ‰€æœ‰æœ‰äº‹ä»¶çš„æ—¥æœŸ
            force_update: å¦‚æœä¸º Trueï¼Œå³ä½¿å·²å­˜åœ¨æ€»ç»“ä¹Ÿä¼šé‡æ–°ç”Ÿæˆ
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼š{date: record_id, ...}
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†æ¯æ—¥æ€»ç»“")
        logger.info("=" * 60)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸåˆ—è¡¨ï¼Œåˆ™è·å–æ‰€æœ‰æ—¥æœŸ
        if date_list is None:
            date_list = self.query_engine.get_distinct_dates()
        
        logger.info(f"ğŸ“… å°†å¤„ç† {len(date_list)} ä¸ªæ—¥æœŸ")
        
        results = {}
        success_count = 0
        skip_count = 0
        
        for idx, target_date in enumerate(date_list, 1):
            logger.info(f"\n[{idx}/{len(date_list)}] å¤„ç†æ—¥æœŸ: {target_date}")
            
            try:
                record_id = self.run_for_date(target_date, force_update=force_update)
                
                if record_id:
                    results[target_date] = record_id
                    success_count += 1
                else:
                    skip_count += 1
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ—¥æœŸ {target_date} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                skip_count += 1
        
        logger.info("\n" + "=" * 60)
        logger.info("æ‰¹é‡å¤„ç†å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ—¥æœŸ")
        logger.info(f"â­ï¸  è·³è¿‡: {skip_count} ä¸ªæ—¥æœŸ")
        logger.info("=" * 60)
        
        return results

