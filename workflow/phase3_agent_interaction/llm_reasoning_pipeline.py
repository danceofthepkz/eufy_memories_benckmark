"""
ç¬¬ä¸‰é˜¶æ®µä¸» Pipeline: LLM_Reasoning_Pipeline
æ•´åˆæ‰€æœ‰4ä¸ªæ¨¡å—ï¼Œå®ç°å®Œæ•´çš„ LLM è¯­ä¹‰ç”Ÿæˆæµç¨‹
"""

import logging
from typing import List, Dict, Any, Optional

from .prompt_engine import PromptEngine
from .llm_gateway import LLMGateway
from .response_validator import ResponseValidator
from .role_classifier import RoleClassifier

logger = logging.getLogger(__name__)


class LLM_Reasoning_Pipeline:
    """ç¬¬ä¸‰é˜¶æ®µï¼šå®è§‚è¯­ä¹‰ç”Ÿæˆ Pipeline"""
    
    def __init__(self,
                 model_name: str = 'gemini-2.5-flash-lite',
                 temperature: float = 0.2,
                 max_output_tokens: int = 256,
                 project_id: Optional[str] = None,
                 location: str = 'us-central1'):
        """
        åˆå§‹åŒ– LLM Reasoning Pipeline
        
        Args:
            model_name: Gemini æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰ï¼Œè¶Šä½è¶Šå®¢è§‚
            max_output_tokens: æœ€å¤§è¾“å‡º token æ•°
            project_id: Google Cloud é¡¹ç›®IDï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            location: Vertex AI åŒºåŸŸ
        """
        logger.info("=" * 60)
        logger.info("åˆå§‹åŒ– LLM Reasoning Pipeline (ç¬¬ä¸‰é˜¶æ®µ)")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.prompt_engine = PromptEngine()                    # æ¨¡å— 2
        self.llm_gateway = LLMGateway(                        # æ¨¡å— 3
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            project_id=project_id,
            location=location
        )
        self.validator = ResponseValidator()                   # æ¨¡å— 4
        self.role_classifier = RoleClassifier()                # è§’è‰²åˆ†ç±»å™¨
        
        logger.info(f"âœ… LLM Reasoning Pipeline åˆå§‹åŒ–å®Œæˆ "
                   f"(æ¨¡å‹: {model_name}, æ¸©åº¦: {temperature})")
    
    def process_events(self, global_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å…¨å±€äº‹ä»¶åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªäº‹ä»¶ç”Ÿæˆè‡ªç„¶è¯­è¨€æ—¥å¿—
        
        Args:
            global_events: Global_Event åˆ—è¡¨ï¼ˆæ¥è‡ª Phase 2ï¼‰
        
        Returns:
            å¤„ç†åçš„ Global_Event åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« 'summary_text' å­—æ®µ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹ LLM è¯­ä¹‰ç”Ÿæˆæµç¨‹")
        logger.info("=" * 60)
        
        if not global_events:
            logger.warning("âš ï¸  è¾“å…¥äº‹ä»¶åˆ—è¡¨ä¸ºç©º")
            return []
        
        logger.info(f"ğŸ“‹ éœ€è¦å¤„ç† {len(global_events)} ä¸ªäº‹ä»¶")
        
        processed_events = []
        
        for idx, event in enumerate(global_events, 1):
            logger.info(f"\n[{idx}/{len(global_events)}] å¤„ç†äº‹ä»¶...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äººç‰©å‡ºç°
            people = event.get('people', [])
            people_info = event.get('people_info', {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é™Œç”Ÿäººï¼ˆå³ä½¿æ²¡æœ‰ person_idï¼‰
            has_strangers = False
            if -1 in people_info:
                has_strangers = people_info[-1].get('has_strangers', False)
            
            if (not people or len(people) == 0) and not has_strangers:
                # å¦‚æœæ²¡æœ‰äººå‡ºç°ï¼ˆåŒ…æ‹¬é™Œç”Ÿäººï¼‰ï¼Œç›´æ¥è¿”å›å›ºå®šå›å¤ï¼Œè·³è¿‡LLMè°ƒç”¨
                logger.info("   æ£€æµ‹åˆ°æ— äººå‡ºç°ï¼Œè·³è¿‡LLMè°ƒç”¨")
                event['summary_text'] = "è¯¥è§†é¢‘ä¸­æ— äººå‡ºç°"
                event['llm_valid'] = True
                event['llm_warnings'] = []
                
                logger.info(f"âœ… äº‹ä»¶ #{idx} å¤„ç†å®Œæˆ")
                logger.info(f"   ç”Ÿæˆæ—¥å¿—: {event['summary_text']}")
                
                processed_events.append(event)
                continue
            
            # å¦‚æœæœ‰é™Œç”Ÿäººä½†æ²¡æœ‰ person_idï¼Œè®°å½•æ—¥å¿—
            if has_strangers:
                stranger_count = people_info[-1].get('stranger_count', 0)
                logger.info(f"   æ£€æµ‹åˆ° {stranger_count} ä¸ªé™Œç”Ÿäººï¼ˆæ—  person_idï¼‰ï¼Œç»§ç»­å¤„ç†")
            
            try:
                # 1. æ„å»º Promptï¼ˆæ¨¡å— 2ï¼‰
                logger.debug("[æ¨¡å— 2] æ„å»º Prompt...")
                prompts = self.prompt_engine.build_full_prompt(event)
                
                # 2. è°ƒç”¨ LLMï¼ˆæ¨¡å— 3ï¼‰
                logger.debug("[æ¨¡å— 3] è°ƒç”¨ LLM API...")
                raw_response = self.llm_gateway.generate(
                    system_prompt=prompts['system_prompt'],
                    user_prompt=prompts['user_prompt']
                )
                
                # è®°å½•åŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                logger.debug(f"LLM åŸå§‹å“åº”: {raw_response[:200]}...")
                
                # 3. éªŒè¯å’Œæ¸…æ´—ï¼ˆæ¨¡å— 4ï¼‰
                logger.debug("[æ¨¡å— 4] éªŒè¯å’Œæ¸…æ´—å“åº”...")
                validation_result = self.validator.validate_and_clean(raw_response, event)
                
                # 4. æ ¹æ®è¡Œä¸ºæ¨æ–­è§’è‰²ï¼ˆæ–°å¢ï¼‰
                logger.debug("[è§’è‰²åˆ†ç±»] æ ¹æ®è¡Œä¸ºæ¨æ–­è§’è‰²...")
                summary_text = validation_result['summary_text']
                people_info = event.get('people_info', {})
                
                # æå–äººç‰©è¡Œä¸ºå¹¶æ¨æ–­è§’è‰²
                behaviors = self.role_classifier.extract_person_behaviors(
                    summary_text, people_info
                )
                
                # æ›´æ–°äººç‰©è§’è‰²
                if behaviors:
                    event = self.role_classifier.update_people_roles(event, behaviors)
                    logger.info(f"   å·²æ ¹æ®è¡Œä¸ºæ›´æ–° {len(behaviors)} ä¸ªäººç‰©çš„è§’è‰²")
                
                # 5. æ·»åŠ ç»“æœåˆ°äº‹ä»¶
                event['summary_text'] = summary_text
                event['llm_valid'] = validation_result['is_valid']
                event['llm_warnings'] = validation_result['warnings']
                
                logger.info(f"âœ… äº‹ä»¶ #{idx} å¤„ç†å®Œæˆ")
                logger.info(f"   ç”Ÿæˆæ—¥å¿—: {validation_result['summary_text']}")
                
                if validation_result['warnings']:
                    logger.warning(f"   âš ï¸  è­¦å‘Š: {validation_result['warnings']}")
                
                processed_events.append(event)
                
            except Exception as e:
                logger.error(f"âŒ äº‹ä»¶ #{idx} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                # ä½¿ç”¨å…œåº•ç”Ÿæˆ
                logger.warning(f"   ä½¿ç”¨å…œåº•ç”Ÿæˆ...")
                fallback_result = self.validator._generate_fallback(event)
                event['summary_text'] = fallback_result['summary_text']
                event['llm_valid'] = False
                event['llm_warnings'] = ['å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å…œåº•ç”Ÿæˆ']
                
                processed_events.append(event)
        
        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… LLM è¯­ä¹‰ç”Ÿæˆå®Œæˆ: {len(processed_events)} ä¸ªäº‹ä»¶")
        logger.info("=" * 60)
        
        # ç»Ÿè®¡ä¿¡æ¯
        valid_count = sum(1 for e in processed_events if e.get('llm_valid', False))
        logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   æ€»äº‹ä»¶æ•°: {len(processed_events)}")
        logger.info(f"   æœ‰æ•ˆç”Ÿæˆ: {valid_count}")
        logger.info(f"   å…œåº•ç”Ÿæˆ: {len(processed_events) - valid_count}")
        
        return processed_events
    
    def process_one_event(self, global_event: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªäº‹ä»¶ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            global_event: Global_Event å¯¹è±¡
        
        Returns:
            å¤„ç†åçš„ Global_Event å¯¹è±¡ï¼ˆåŒ…å« 'summary_text' å­—æ®µï¼‰
        """
        results = self.process_events([global_event])
        return results[0] if results else global_event

