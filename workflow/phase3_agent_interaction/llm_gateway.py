"""
æ¨¡å— 3: LLM å®¢æˆ·ç«¯ç½‘å…³ (LLM Client Gateway)
èŒè´£ï¼šä¸ Google Gemini API è¿›è¡Œç¨³å®šçš„äº¤äº’
"""

import os
import logging
from typing import Optional, Dict, Any
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

import logging

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ Vertex AI
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel
    VERTEX_AI_AVAILABLE = True
except ImportError:
    VERTEX_AI_AVAILABLE = False
    logger.warning("âš ï¸  vertexai æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨ Gemini API")


class LLMGateway:
    """LLM å®¢æˆ·ç«¯ç½‘å…³"""
    
    def __init__(self, 
                 model_name: str = 'gemini-2.5-flash-lite',
                 temperature: float = 0.2,
                 max_output_tokens: int = 256,
                 project_id: Optional[str] = None,
                 location: str = 'us-central1'):
        """
        åˆå§‹åŒ– LLM ç½‘å…³
        
        Args:
            model_name: Gemini æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰ï¼Œè¶Šä½è¶Šå®¢è§‚
            max_output_tokens: æœ€å¤§è¾“å‡º token æ•°
            project_id: Google Cloud é¡¹ç›®IDï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            location: Vertex AI åŒºåŸŸ
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens
        
        # è·å–é¡¹ç›®ID
        if project_id is None:
            project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
            if not project_id:
                raise ValueError("æ— æ³•ç¡®å®šé¡¹ç›®IDï¼Œè¯·è®¾ç½® GOOGLE_CLOUD_PROJECT ç¯å¢ƒå˜é‡")
        
        self.project_id = project_id
        self.location = location
        
        # åˆå§‹åŒ– Vertex AI
        if VERTEX_AI_AVAILABLE:
            try:
                vertexai.init(project=project_id, location=location)
                self.model = GenerativeModel(model_name)
                logger.info(f"âœ… LLM ç½‘å…³åˆå§‹åŒ–æˆåŠŸ: {model_name} (é¡¹ç›®: {project_id}, åŒºåŸŸ: {location})")
            except Exception as e:
                logger.error(f"âŒ Vertex AI åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        else:
            logger.error("âŒ vertexai æœªå®‰è£…ï¼Œæ— æ³•åˆå§‹åŒ– LLM ç½‘å…³")
            raise ImportError("è¯·å®‰è£… google-cloud-aiplatform: pip install google-cloud-aiplatform")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,))
    )
    def generate(self, system_prompt: str, user_prompt: str) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆæ–‡æœ¬
        
        Args:
            system_prompt: System Prompt
            user_prompt: User Prompt
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if not VERTEX_AI_AVAILABLE:
            raise ImportError("vertexai æœªå®‰è£…")
        
        try:
            # ç»„åˆå®Œæ•´çš„ Prompt
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            logger.debug(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ° Gemini API (æ¨¡å‹: {self.model_name})")
            logger.debug(f"   Prompt é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
            
            # è°ƒç”¨æ¨¡å‹
            generation_config = {
                'temperature': self.temperature,
                'max_output_tokens': self.max_output_tokens,
            }
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            # æå–æ–‡æœ¬
            if hasattr(response, 'text') and response.text:
                generated_text = response.text.strip()
                logger.debug(f"ğŸ“¥ æ”¶åˆ°å“åº”: {len(generated_text)} å­—ç¬¦")
                return generated_text
            else:
                logger.warning("âš ï¸  API å“åº”ä¸ºç©º")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def generate_simple(self, prompt: str) -> str:
        """
        ç®€åŒ–ç‰ˆç”Ÿæˆæ–¹æ³•ï¼ˆåªä½¿ç”¨ User Promptï¼Œæ—  System Promptï¼‰
        
        Args:
            prompt: å®Œæ•´çš„ Prompt æ–‡æœ¬
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        return self.generate("", prompt)

